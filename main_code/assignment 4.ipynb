{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-07T17:55:30.267208Z",
     "start_time": "2025-09-07T17:55:30.042294Z"
    }
   },
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T17:54:44.388914Z",
     "start_time": "2025-09-07T17:54:43.932744Z"
    }
   },
   "cell_type": "markdown",
   "source": "# Scrape Books data",
   "id": "4b4d75b33413eac0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T17:57:27.432413Z",
     "start_time": "2025-09-07T17:56:09.519858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_url = \"https://books.toscrape.com/catalogue/page-{}.html\"\n",
    "books = []\n",
    "\n",
    "for page in range(1, 51):\n",
    "    url = base_url.format(page)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    for book in soup.find_all('article', class_='product_pod'):\n",
    "        title = book.h3.a['title']\n",
    "        price = book.find('p', class_='price_color').text.strip()\n",
    "        availability = book.find('p', class_='instock availability').text.strip()\n",
    "        star_rating = book.p['class'][1]\n",
    "\n",
    "        books.append([title, price, availability, star_rating])\n",
    "\n",
    "df_books = pd.DataFrame(books, columns=['Title', 'Price', 'Availability', 'Star Rating'])\n",
    "df_books.to_csv(\"books.csv\", index=False)\n",
    "print(\"books.csv saved\")\n"
   ],
   "id": "572dd60a33a1e8ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books.csv saved\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Scrape IMDB Top 250 Movies",
   "id": "741a6f96db18105a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T17:57:39.218284Z",
     "start_time": "2025-09-07T17:57:27.459670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url = \"https://www.imdb.com/chart/top/\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "movies = []\n",
    "rows = driver.find_elements(By.XPATH, '//tbody[@class=\"lister-list\"]/tr')\n",
    "\n",
    "for row in rows:\n",
    "    rank = row.find_element(By.XPATH, './/td[@class=\"titleColumn\"]').text.split('.')[0]\n",
    "    title = row.find_element(By.XPATH, './/td[@class=\"titleColumn\"]/a').text\n",
    "    year = row.find_element(By.XPATH, './/td[@class=\"titleColumn\"]/span').text.strip(\"()\")\n",
    "    rating = row.find_element(By.XPATH, './/td[@class=\"ratingColumn imdbRating\"]/strong').text\n",
    "    movies.append([rank, title, year, rating])\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df_imdb = pd.DataFrame(movies, columns=['Rank', 'Title', 'Year', 'Rating'])\n",
    "df_imdb.to_csv(\"imdb_top250.csv\", index=False)\n",
    "print(\"imdb_top250.csv saved\")\n"
   ],
   "id": "1d8ce89ec92b45d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb_top250.csv saved\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Scrape Weather Data",
   "id": "3c9573b019e1519d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T17:57:45.083781Z",
     "start_time": "2025-09-07T17:57:44.178608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url = \"https://www.timeanddate.com/weather/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "cities_data = []\n",
    "table = soup.find(\"table\", class_=\"zebra\")\n",
    "\n",
    "if table:\n",
    "    for row in table.find_all(\"tr\")[1:]:\n",
    "        cols = row.find_all(\"td\")\n",
    "        if len(cols) >= 3:\n",
    "            city = cols[0].text.strip()\n",
    "            temp = cols[1].text.strip()\n",
    "            condition = cols[2].text.strip()\n",
    "            cities_data.append([city, temp, condition])\n",
    "\n",
    "    df_weather = pd.DataFrame(cities_data, columns=[\"City\", \"Temperature\", \"Condition\"])\n",
    "    df_weather.to_csv(\"weather.csv\", index=False)\n",
    "    print(\"weather.csv saved with\", len(df_weather), \"rows\")\n",
    "else:\n",
    "    print(\"Weather table not found! Verify the table structure and class name.\")\n"
   ],
   "id": "179cf886b1afe995",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weather.csv saved with 70 rows\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
